{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Storyline\r\n",
    "\r\n",
    "This is a step by step story how this model works."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from collections import defaultdict\r\n",
    "from os import path\r\n",
    "\r\n",
    "from allennlp.commands.elmo import ElmoEmbedder\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "from et4el.models import ELMoWeightedSum, BiLSTM, CNN, SelfAttentiveSum\r\n",
    "from et4el.utils import Mention, Mentions, MentionHandler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Load Elmo Embedder\r\n",
    "ELMO_OPTIONS_FILE = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\r\n",
    "ELMO_WEIGHT_FILE = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\r\n",
    "cuda_device = -1 if device.type == \"cpu\" else device.index or 0\r\n",
    "embedder = ElmoEmbedder(ELMO_OPTIONS_FILE, ELMO_WEIGHT_FILE, cuda_device=cuda_device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Define example Mentions -> batch\r\n",
    "mention_a = Mention(\"Ant\", \"Tired of dealing with a growing jumble of build difficulties, developer James Davidson created\", \", a build tool for Java projects.\")\r\n",
    "mention_b = Mention(\"Washington\", \"In the northwestern US state of\", \", there are typically two harvests: one from late April to May and another from late June into July\")\r\n",
    "mentions = Mentions([mention_a, mention_b])\r\n",
    "# Prepare (trim) mentions\r\n",
    "mentions = MentionHandler(10, 50).prepare_mentions(mentions)\r\n",
    "bsz = len(mentions)\r\n",
    "mentions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Mentions(Ant, Washington)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Define hyperparameters like dimensions and dropouts\r\n",
    "mention_dropout_rate=0.5\r\n",
    "input_dropout_rate=0.5\r\n",
    "rnn_dim=50\r\n",
    "cnn_dim=50\r\n",
    "mask_dim=50\r\n",
    "attention_dim=100\r\n",
    "answer_num=60000\r\n",
    "max_mention_length=10\r\n",
    "max_context_length=50\r\n",
    "threshold=0.58\r\n",
    "\r\n",
    "embeddings_dim = 1024\r\n",
    "output_dim = 2 * rnn_dim + embeddings_dim + cnn_dim\r\n",
    "combined_dim = embeddings_dim + mask_dim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Embed mentions\r\n",
    "embs = embedder.embed_batch(mentions.tokens)\r\n",
    "embs[0].shape, embs[1].shape # embs has the embeddings for each mention at the respective index"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((3, 22, 1024), (3, 26, 1024))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Sentence Embeddings\r\n",
    "max_tokens_length = max(mentions.tokens_lengths)\r\n",
    "sentence_embeddings = torch.zeros([bsz, 3, max_tokens_length, embeddings_dim], device=device)\r\n",
    "for i, emb in enumerate(embs):\r\n",
    "    _, token_length, _ = emb.shape\r\n",
    "    sentence_embeddings[i, :, :token_length, :] = torch.from_numpy(emb)\r\n",
    "sentence_embeddings.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 26, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Mention Embeddings\r\n",
    "max_mention_tokens_length = max([mention.mention.count(\" \") + 1 for mention in mentions])\r\n",
    "mention_embeddings = torch.zeros([bsz, 3, max_mention_tokens_length, embeddings_dim], device=device)\r\n",
    "for i, (emb, mention) in enumerate(zip(embs, mentions)):\r\n",
    "    start_ind, end_ind = mention.borders\r\n",
    "    mention_length = end_ind - start_ind\r\n",
    "    mention_embeddings[i, :, :mention_length, :] = torch.from_numpy(emb[:, start_ind:end_ind, :])\r\n",
    "mention_embeddings.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence encoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Define networks\r\n",
    "weighted_sum = ELMoWeightedSum().to(device)\r\n",
    "location_LNN = nn.Linear(4, mask_dim).to(device)\r\n",
    "input_dropout = nn.Dropout(input_dropout_rate).to(device)\r\n",
    "bi_lstm = BiLSTM(combined_dim, rnn_dim).to(device)\r\n",
    "attentive_sum = SelfAttentiveSum(rnn_dim * 2, attention_dim).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "weighted_embeddings = weighted_sum(sentence_embeddings)\r\n",
    "weighted_embeddings.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 26, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "max_seq_length = max(mentions.tokens_lengths)\r\n",
    "bsz = len(mentions)\r\n",
    "location_tokens = torch.zeros([bsz, max_seq_length, 4], device=device)\r\n",
    "for i, mention in enumerate(mentions):\r\n",
    "    start_ind, end_ind = mention.borders\r\n",
    "    location_tokens[i, :start_ind, 0] = 1.0\r\n",
    "    location_tokens[i, start_ind, 1] = 1.0\r\n",
    "    location_tokens[i, start_ind + 1:end_ind, 2] = 1.0\r\n",
    "    location_tokens[i, end_ind:mention.tokens_length, 3] = 1.0\r\n",
    "location_tokens = location_tokens.view(-1, 4)\r\n",
    "location_tokens.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([52, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "location_mask = location_LNN(location_tokens)\r\n",
    "location_mask = location_mask.view(weighted_embeddings.size()[0], -1, mask_dim)\r\n",
    "location_mask.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 26, 50])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "weighted_embeddings = torch.cat((weighted_embeddings, location_mask), 2)\r\n",
    "weighted_embeddings = input_dropout(weighted_embeddings)\r\n",
    "weighted_embeddings.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 26, 1074])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sequence_lengths = torch.tensor(mentions.tokens_lengths, device=device)\r\n",
    "sequence_rep = bi_lstm(weighted_embeddings, sequence_lengths)\r\n",
    "sequence_rep = attentive_sum(sequence_rep)\r\n",
    "sequence_rep.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 100])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mention Encoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Define Networks\r\n",
    "weighted_sum = ELMoWeightedSum().to(device)\r\n",
    "input_dropout = nn.Dropout(mention_dropout_rate).to(device)\r\n",
    "bi_lstm = BiLSTM(embeddings_dim, embeddings_dim // 2).to(device)\r\n",
    "attentive_sum = SelfAttentiveSum(embeddings_dim, attention_dim).to(device)\r\n",
    "cnn = CNN(cnn_dim).to(device)\r\n",
    "\r\n",
    "# Load char dictionary\r\n",
    "PATH_TO_CHARDICT = path.normpath(\"../et4el/ontology/char_vocab.english.txt\")\r\n",
    "char_dict = defaultdict(int)\r\n",
    "char_vocab = [u\"<unk>\"]\r\n",
    "with open(PATH_TO_CHARDICT, encoding=\"utf-8\") as f:\r\n",
    "    char_vocab.extend(c.strip() for c in f.readlines())\r\n",
    "    char_dict.update({c: i for i, c in enumerate(char_vocab)})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "weighted_embeddings = weighted_sum(mention_embeddings)\r\n",
    "weighted_embeddings = input_dropout(weighted_embeddings)\r\n",
    "weighted_embeddings.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "mention_lengths = torch.tensor([mention.mention.count(\" \") + 1 for mention in mentions], device=device)\r\n",
    "mention_word = bi_lstm(weighted_embeddings, mention_lengths)\r\n",
    "mention_word = attentive_sum(mention_word)\r\n",
    "mention_word.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def pad_slice(seq, seq_length, pad_token=\"<none>\"):\r\n",
    "    return seq + ([pad_token] * (seq_length - len(seq)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "mentions_characters = [[char_dict[x] for x in list(mention.mention)] for mention in mentions]\r\n",
    "max_span_chars = max(max(len(characters) for characters in mentions_characters), 5)\r\n",
    "mentions_characters = [\r\n",
    "    pad_slice(characters, max_span_chars, pad_token=0) for characters in mentions_characters\r\n",
    "]\r\n",
    "mention_chars = torch.tensor(mentions_characters, dtype=torch.int64, device=device)\r\n",
    "mention_chars = cnn(mention_chars)\r\n",
    "mention_chars.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 50])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "mention_rep = torch.cat((mention_word, mention_chars), 1)\r\n",
    "mention_rep.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 1074])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "linear = nn.Linear(output_dim, answer_num, bias=False).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "representation = torch.cat((sequence_rep, mention_rep), 1)\r\n",
    "logits = linear(representation)\r\n",
    "logits.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 60000])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "PATH_TO_VOCAB = \"../et4el/ontology/conll_categories.txt\"\r\n",
    "with open(PATH_TO_VOCAB, encoding=\"utf-8\") as f:\r\n",
    "    text = [x.strip() for x in f.readlines()]\r\n",
    "    text = text[:answer_num]\r\n",
    "    file_content = dict(zip(text, range(len(text))))\r\n",
    "answer2id = file_content\r\n",
    "id2answer = {v: k for k, v in answer2id.items()}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Decode predictions to categories\r\n",
    "outputs = torch.sigmoid(logits)\r\n",
    "predictions = []\r\n",
    "for output in outputs:\r\n",
    "    output_indices = (output > threshold).nonzero().squeeze(1)\r\n",
    "    if len(output_indices) == 0:\r\n",
    "        output_indices = torch.argmax(output, dim=0, keepdim=True)\r\n",
    "    predicted_categories = [(id2answer[i.item()], output[i].item()) for i in output_indices]\r\n",
    "    predictions.append(predicted_categories)\r\n",
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[('hamlets in devon', 0.5857832431793213)],\n",
       " [('sport in birmingham, west midlands', 0.5891037583351135),\n",
       "  ('former new york central railroad stations', 0.5822668671607971),\n",
       "  ('freedesktop.org', 0.5833342671394348),\n",
       "  ('by universal television', 0.5883475542068481),\n",
       "  ('populated coastal places in new zealand', 0.5829136967658997),\n",
       "  ('of the amazon', 0.5838868618011475),\n",
       "  ('economy of the midwestern united states', 0.6019372940063477),\n",
       "  ('1st-century establishments', 0.5869855880737305),\n",
       "  ('towns in boone county, arkansas', 0.5844034552574158),\n",
       "  ('bayfield-class attack transports', 0.5825484991073608),\n",
       "  ('historiography of england', 0.5862821936607361),\n",
       "  ('economic nationalism', 0.583835780620575),\n",
       "  ('buildings and structures in sydney', 0.5802591443061829),\n",
       "  ('cities in wayne county, michigan', 0.5895113348960876),\n",
       "  ('19th-century in mexico', 0.587650716304779),\n",
       "  ('czech-american culture in kansas', 0.5931905508041382),\n",
       "  ('sc freiburg', 0.6039355397224426),\n",
       "  ('in palestinian territories', 0.5820906162261963),\n",
       "  ('defunct mining companies of canada', 0.5839600563049316),\n",
       "  ('of the isle of wight', 0.5801122188568115),\n",
       "  ('rugby clubs established in 1865', 0.5818402171134949),\n",
       "  ('songs written by roger troutman', 0.5832414031028748),\n",
       "  ('left-wing militant groups', 0.5865589380264282),\n",
       "  ('of the sopranos characters', 0.58189857006073),\n",
       "  ('1971 disestablishments in africa', 0.5877971053123474),\n",
       "  ('attacks in the united states in 2007', 0.5816826820373535),\n",
       "  ('annual sporting events', 0.6011105179786682),\n",
       "  ('northamptonshire', 0.5818365812301636)]]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## With own Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import torch\r\n",
    "from et4el.encoder import MentionEncoder, SentenceEncoder, SimpleDecoder\r\n",
    "from et4el.embedder import ELMoPretrainedEmbedder\r\n",
    "from et4el.utils import Mentions, Mention, MentionHandler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "pre_handler = MentionHandler(max_mention_length, max_context_length)\r\n",
    "embedder = ELMoPretrainedEmbedder(device).to(device)\r\n",
    "sentence_encoder = SentenceEncoder(input_dropout_rate, rnn_dim, embeddings_dim, mask_dim, attention_dim).to(device)\r\n",
    "mention_encoder = MentionEncoder(mention_dropout_rate, cnn_dim, embeddings_dim, attention_dim).to(device)\r\n",
    "decoder = SimpleDecoder(output_dim, answer_num).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "mentions = pre_handler.prepare_mentions(mentions)\r\n",
    "embeddings = embedder.embed(mentions)\r\n",
    "sentence_rep = sentence_encoder(mentions, embeddings)\r\n",
    "mention_rep = mention_encoder(mentions, embeddings)\r\n",
    "\r\n",
    "output = torch.cat((sentence_rep, mention_rep), 1)\r\n",
    "logits = decoder(output)\r\n",
    "logits.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 60000])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('et4el-lightning': conda)"
  },
  "interpreter": {
   "hash": "c9abe7bae20303d22f6787541574ad571c96a9ca91520a95e8b77ca70a0fe0fc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}